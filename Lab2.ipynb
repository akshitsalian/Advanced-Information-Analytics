{"cells":[{"cell_type":"code","source":["# ALWAYS IMPORT THESE PACKAGES AT THE BEGINNING\nfrom __future__ import division, absolute_import\nfrom pyspark.sql import Row\nfrom pyspark.ml import regression\nfrom pyspark.ml import feature\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import functions as fn"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["# Part 1. MapReduce"],"metadata":{}},{"cell_type":"markdown","source":["1.1 (30 pts)\n\nSometimes, it is useful to summarize a set of numbers into binned frequencies. For example, to understand the distribution of age in a population, we would like to know how many people are between 0-20, how many between 20-40, how many between 40-60, and so on.\n\nCreate a map-reduce job with Spark that takes an RDD with each input element being a tuple (label, value). For each label, the map-reduce job must generate a list with the counts of how many values fall in each of the following ranges: `x < 0`, `0<= x <20`, `20 <= x < 40`, `40 <= x < 60`, and `60 <= x`. This is, the map-reduce job must generate a list of length 5 where the first element contains the frequency where `x < 0`, the second element contains the number of values where `0 <= x < 20`, and so on. You only need one map and one reduce operation. Use vanilla Python lists.\n\nFor example, for the following RDD\n\n```\nrdd = sc.parallelize([('healthy', -100),\n('healthy', 0.),\n('healthy', 12.),\n('sick', 10.),\n('sick', 58.),\n('sick', 60),\n('sick', 100),\n])\n```\n\nThe result of `rdd.map(map_bin).reduceByKey(reduce_bin).collect()` should be:\n\n```\n[('healthy', [1, 2, 0, 0, 0]), ('sick', [0, 1, 0, 1, 2])]\n```"],"metadata":{}},{"cell_type":"code","source":["# define map and reduce functions here\ndef map_bin(x):\n  L=[0,0,0,0,0]\n  \n  if(x[0]<0):\n    L[0]=L[0]+1\n  elif(x[1]>=0 and x[1]<20):\n    L[1]=L[1]+1\n  elif(x[1]>=20 and x[1]<40):\n    L[2]=L[2]+1\n  elif(x[1]>=40 and x[1]<60):\n    L[3]=L[3]+1\n  elif(x[1]>=60):\n    L[4]=L[4]+1\n  return [x[0],L]\n\ndef reduce_bin(v1, v2):\n  \n  return [x+L for x,L in zip(v1,v2)]"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# apply your map-reduce job to the following RDD\nrdd = sc.parallelize([('low', 8), ('low', -2), ('low', -7), ('low', 11), ('low', 5), ('low', -8), ('low', 14), ('low', 9), ('low', 8), ('low', -6), ('low', 11), ('low', 8), ('low', 13), ('low', -10), ('low', 10), ('low', -4), ('low', -5), ('low', 6), ('low', 13), ('low', -3), ('unknown', 104), ('unknown', 130), ('unknown', 57), ('unknown', 50), ('unknown', 12), ('unknown', 110), ('unknown', 65), ('unknown', 66), ('unknown', 47), ('unknown', 96), ('high', 45), ('high', 44), ('high', 50), ('high', 45), ('high', 50), ('high', 44), ('high', 45), ('high', 46), ('high', 43), ('high', 52), ('high', 51), ('high', 46), ('high', 52), ('high', 53), ('high', 50), ('middle', 19), ('middle', 25), ('middle', 27), ('middle', 40), ('middle', 13), ('middle', 15), ('middle', 27), ('middle', 26), ('middle', 19), ('middle', 23)])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["rdd.map(map_bin).reduceByKey(reduce_bin).collect()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["# Part 2: Preprocess data and create dataframes"],"metadata":{}},{"cell_type":"markdown","source":["Sometimes, we must preprocess messy data through several steps. \n\nConsider the dataset in `/databricks-datasets/sample_logs`. This dataset contains access logs to an Apache webserver. Take for example the following line\n\n`3.3.3.3 - user1 [21/Jun/2014:10:00:00 -0700] \"GET /endpoint_27 HTTP/1.1\" 200 21`\n\nwhere the format is as follows:\n\n1. the IP\n2. user name if authenticated or `-` if not authenticated\n3. Time stamp of the access with time zone\n4. Method, endpoint, and protolcol\n5. Request status. See more here https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html\n6. Size of the object returned to client in bytes\n\nYou can read more details about the structure of Apache logs here https://httpd.apache.org/docs/2.4/logs.html#accesslog"],"metadata":{}},{"cell_type":"code","source":["# we will read the logs into an RDD\nrdd = sc.textFile('/databricks-datasets/sample_logs')"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["rdd.take(10)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["2.1 (10 pts) **Transform the RDD** Transform the original RDD from line into list of length 8 with the following structure:\n\n1. Index 0: String with the IP\n2. Index 1: A number 1 if there is a username and 0 if there no username (i.e., `-`)\n3. Index 2: A number representing the month, starting from `Jan` = 0, `Feb` = 1, ..., `Dec` = 11\n4. Index 3: A number 1 if the method is `GET` and 0 otherwise.\n5. Index 4: String with the endpoint (e.g., `\"/endpoint_27\"`)\n6. Index 5: String with the protocol (e.g., `\"HTTP/1.1\"`)\n7. Index 6: Integer with the response (e.g., `200`)\n8. Index 7: Integer with response size (e.g., `21`)\n\nHints: You can transform a string into an integer by applying the function `int`. For example, `int(\"21\")` becomes `21`. Also, remember that to concatenate strings you can use the `+` operator"],"metadata":{}},{"cell_type":"code","source":["def split_line(line):\n  L1=[]\n  L2=[0,0,0,0,0,0,0,0]\n  month_data=dict(Jan=0,Feb=1,Mar=2,Apr=3,May=4,Jun=5,Jul=6,Aug=7,Sep=8,Oct=9,Nov=10,Dec=11)\n  \n  L1=line.split(\" \")\n  \n  L2[0]=L1[0]\n  if(L1[2]=='-'):\n    L2[1]=0\n  else:\n    L2[1]=1\n    \n  month=L1[3].split(\"/\")[1]\n  L2[2]=month_data[month]\n  \n  if(L1[5]==\"\\\"GET\"):\n    L2[3]=1\n  else:\n    L2[3]=0\n    \n  L2[4]=L1[6]\n  L2[5]=L1[7]\n  L2[6]=int(L1[8])\n  L2[7]=int(L1[9])\n  return L2"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["rdd.take(10)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["rdd.map(split_line).take(10)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["2.2 (10 pts) **Exploratory analysis** Sometimes, we want to understand where most errors happen. We will define an \"error\" as any response code greater or equal to 300 and not error as everything else. Using map-reduce, estimate the frequency of errors per month and user presence, with month and user presence encoded as in question 2.1. *Hint*: The key should be a tuple `(has_user, n_month)` where `has_user` is 0 or 1 and `n_month` is in `[0, ..., 11]`. You should use the function from question 2.1."],"metadata":{}},{"cell_type":"code","source":["def map_explore(l):\n  L1=0\n  if(l[6]>=300):\n    L1=1\n  \n  \n  return ((l[1],l[2]),L1) \n\ndef reduce_explore(v1, v2):\n  return v1+v2"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["rdd.map(split_line).map(map_explore).reduceByKey(reduce_explore).collect()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["2.3 (10 pts) **From RDD to DataFrame** From the RDD created in question 2.1, create a DataFrame by using the `toDF()` method of an RDD. This RDD should have a set of `Row` objects with the following names for the fields:\n\n1. Index 0: ip\n2. Index 1: has_user\n3. Index 2: n_month\n4. Index 3: is_get\n5. Index 4: endpoint\n6. Index 5: protocol\n7. Index 6: response\n8. Index 7: size"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["df = rdd.map(split_line).map(lambda x:Row(IP=x[0],has_user=x[1],n_month=x[2],is_get=x[3],endpoint=x[4],protocol=x[5],response=x[6],size=x[7])).toDF()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["2.4 (10 pts) **Linear regression** Use the dataframe created in question 2.3 to build a model that uses `has_user`, `n_month`, and `is_get` to predict `size`. Further, print the parameters of the model and estimate the mean squared error of the model on the training data. *Hint:* Use a `Pipeline` of `VectorAssembler` and a `LinearRegression`."],"metadata":{}},{"cell_type":"code","source":["va = feature.VectorAssembler(inputCols=['has_user','n_month','is_get'], outputCol='features')\nlr = regression.LinearRegression(featuresCol='features', labelCol='size')\npipe = Pipeline(stages=[va, lr])"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["pipe_model = pipe.fit(df)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# define symbolic MSE expression using fn package\nmse = fn.avg((fn.col('size') - fn.col('prediction'))**2)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["pipe_model.transform(df).select(mse).show()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Part 3: Dataframe manipulation and model comparison"],"metadata":{}},{"cell_type":"markdown","source":["For this question, we will use the TPC-H schema, which is a standard schema used for benchmarking big data SQL engines. The full schema is displayed below. An arrow indicates how columns of a table are related to other tables. For example, the column `ORDERKEY` in the dataframe `LINEITEM` refers to the dataframe `ORDERS`. This means that for each line item we can get the order details by joining `LINEITEM` with `ORDERS` by using `ORDERKEY` as the join key."],"metadata":{}},{"cell_type":"markdown","source":["![](https://docs.snowflake.net/manuals/_images/sample-data-tpch-schema.png)"],"metadata":{}},{"cell_type":"markdown","source":["We will only create the DataFrames for customer (`customer_df`), orders (`order_df`), and line item (`lineitem_df`):"],"metadata":{}},{"cell_type":"code","source":["customer_df = spark.read.csv('dbfs:/databricks-datasets/tpch/data-001/customer/', sep='|').\\\n  selectExpr('_c0 as CUSTKEY', \n             '_c1 as NAME',\n             '_c2 as ADDRESS',\n             'cast(_c3 as float) as NATIONKEY',\n             '_c4 as PHONE',\n             'cast(_c5 as float) as ACCTBAL',\n             '_c6 as MKTSEGMENT',\n             '_c7 as COMMMENT')\n\norder_df = spark.read.csv('dbfs:/databricks-datasets/tpch/data-001/orders/', sep='|').\\\n  selectExpr('_c0 as ORDERKEY',\n             '_c1 as CUSTKEY',\n             '_c2 as ORDERSTATUS',\n             'cast(_c3 as float) as TOTALPRICE',\n             '_c4 as ORDERDATE',\n             '_c5 as ORDER_PRIORITY',\n             '_c6 as CLERK',\n             '_c7 as SHIP_PRIORITY',\n             '_c8 as COMMENT')\n\nlineitem_df = spark.read.csv('dbfs:/databricks-datasets/tpch/data-001/lineitem/', sep='|').\\\n  selectExpr('_c0 as ORDERKEY',\n             '_c1 as PARTKEY',\n             '_c2 as SUPPKEY',\n             '_c3 as LINENUMBER',\n             '_c4 as QUANTITY',\n             '_c5 as EXTENDEDPRICE',\n             '_c6 as DISCOUNT',\n             '_c7 as TAX',\n             '_c8 as RETURNFLAG',\n             '_c9 as LINESTATUS',\n             '_c10 as SHIPDATE',\n             '_c11 as COMMITDATE',\n             '_c12 as RECEIPTDATE',\n             '_c13 as SHIPINSTRUCT',\n             '_c14 as SHIPMODE',\n             '_c15 as COMMENT')"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["3.1 (5 pts) **Outer joins** Compute how many customers do not have orders. *Hint*: Join the customer and order dataframes and use the appropriate `how` option. Then, select the cases where the `orderkey` is null using the appropriate function from the package `fn` (i.e., no matching order is available). Count the number of rows in that resulting dataframe."],"metadata":{}},{"cell_type":"code","source":["customer_df.join(order_df,on='CUSTKEY',how='left').filter(fn.isnull('ORDERKEY')).select(fn.count('CUSTKEY')).show()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["3.2 (5 pts) **Summary stats** Estimate how much a customer pay in taxes on average. *Hint:* Link customer, order, and lineitem dataframes."],"metadata":{}},{"cell_type":"code","source":["customer_df.join(order_df,on='CUSTKEY',how='inner').join(lineitem_df,on='ORDERKEY',how='left').groupBy('CUSTKEY').agg(fn.avg('TAX')).show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["3.3 (20 pts) **Assessing model accuracy** Use training, validation, and testing splits to build a model that predicts the order's total price (`TOTALPRICE`) using the following models:\n\n1. No features (only intercept)\n2. Customer account balance (`ACCTBAL`)\n3. Customer account balance (`ACCTBAL`) and nation key (`NATIONKEY`)\n\nReport the **root mean squared error** (RMSE) of each model for validation (15 pts) and the RMSE of the best model on testing (5 pts). The RMSE is simply the square root of the MSE.\n\n*Hint*: You need to build three pipelines and use different  vector assemblers and linear regressions accordingly. Use the `randomSplit` dataframe method to use 60% for training, 30% for validation, and 10% for testing."],"metadata":{}},{"cell_type":"code","source":["data = customer_df.join(order_df,on='CUSTKEY')"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["training_df, validation_df, testing_df = data.randomSplit([0.6, 0.3, 0.1])"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["va1=feature.VectorAssembler(inputCols=[], outputCol='features')\nva2=feature.VectorAssembler(inputCols=['ACCTBAL'], outputCol='features')\nva3=feature.VectorAssembler(inputCols=['ACCTBAL','NATIONKEY'], outputCol='features')\n\nlr = regression.LinearRegression(featuresCol='features', labelCol='TOTALPRICE')"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["pipe1=Pipeline(stages=[va1,lr])\npipe2=Pipeline(stages=[va2,lr])\npipe3=Pipeline(stages=[va3,lr])"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["model1 = pipe1.fit(training_df)\nmodel2 = pipe2.fit(training_df)\nmodel3 = pipe3.fit(training_df)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# symbolically define RMSE\nrmse = fn.sqrt(fn.avg((fn.col('TOTALPRICE') - fn.col('prediction'))**2))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# model selection\nmodel1.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["model2.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["model3.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# estimate generalization error\nmodel1.transform(testing_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"name":"homework-2-no-key (1)","notebookId":2197318770033936},"nbformat":4,"nbformat_minor":0}

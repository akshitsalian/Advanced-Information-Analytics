{"cells":[{"cell_type":"markdown","source":["# Homework 3: Sarcasm prediction\n\nIn this homework, we are going to detect sarcasm."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import feature\nfrom pyspark.ml import classification\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import evaluation\nfrom pyspark.sql import functions as fn\nimport pandas as pd\n\nimport requests\nfrom pyspark.sql import Row\nrdd_raw = sc.parallelize(requests.get('https://raw.githubusercontent.com/daniel-acuna/python_data_science_intro/master/data/sarcasm-dataset.txt').content.split('\\n'))\nsarcasm_df = rdd_raw.filter(lambda x: len(x)>1).map(lambda x: Row(text=x[:-2], sarcastic=int(x[-1]))).toDF()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["display(sarcasm_df)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# the data is almost balanced\nsarcasm_df.groupBy('sarcastic').count().show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# use these splits throught the notebook\ntraining, validation, testing = sarcasm_df.randomSplit([0.6, 0.3, 0.1])\nsubtraining1, subtraining2 = training.randomSplit([0.6, 0.4])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["** Question 1 (30 pts)** This dataset is based on tweets and therefore sarcasm is sometimes represented by the hashtag \"#not\". Build a simple classifier that predicts sarcasm when the text contains such hashtag. Estimate the accuracy of the classifier (you don't need to split the data into training because there is no training!) **Hint**: This can be solved using the function `fn.instr` to check if a string is inside another. This function returns 0 if nothing is found."],"metadata":{}},{"cell_type":"code","source":["a=fn.instr(sarcasm_df['text'],'#not')"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["display(sarcasm_df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["predicted_sarcasm_df"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# your code here\npredicted_sarcasm_df=sarcasm_df.\\\nwithColumn(\"predicted\",fn.when(fn.instr(sarcasm_df['text'],'#not' )!=0,1).otherwise(0))\npredicted_sarcasm_df.show(10)\n\n#predicting accuracy\npredicted_sarcasm_df.select(fn.expr('int(sarcastic = predicted)').alias('correct')).select(fn.avg('correct')).show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["** Question 2 (40 pts)** Build and evaluate the performance of a classifiers of sarcasm using elastic net regularized logistic regression on the TFIDF representation of the text. Compare three models with \\\\( \\alpha \\in (0, 0.05, 0.1 ) \\\\) and fixed \\\\( \\lambda = 0.1 \\\\) Use accuracy to evaluate generalization performance. Using the best model, show the words that have the highest and lowest weights on the prediction (don't show words with weights 0) *Hint*: Follow the steps of the sentiment analysis notebook"],"metadata":{}},{"cell_type":"code","source":["# your code here\nfrom pyspark.ml.feature import RegexTokenizer\nfrom pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.feature import IDF\nfrom pyspark.ml.classification import LogisticRegression\n#splitting the words in  a review\ntokenizer = RegexTokenizer().setGaps(False)\\\n  .setPattern(\"\\\\p{L}+\")\\\n  .setInputCol(\"text\")\\\n  .setOutputCol(\"words\")\n#obtaining stop words\nimport requests\nstop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n#filtering the words by removing stop words\nfrom pyspark.ml.feature import StopWordsRemover\nsw_filter = StopWordsRemover()\\\n  .setStopWords(stop_words)\\\n  .setCaseSensitive(False)\\\n  .setInputCol(\"words\")\\\n  .setOutputCol(\"filtered\")\n#remove words that appear in 5 docs or less\ncv = CountVectorizer(minTF=1., minDF=5., vocabSize=2**17)\\\n  .setInputCol(\"filtered\")\\\n  .setOutputCol(\"tf\")\n#creating a pipelined transformer\ncv_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv]).fit(sarcasm_df)\n#making the transformation between the raw text and the counts\ncv_pipeline.transform(sarcasm_df).show(5)\n#building a pipeline to take the output of the previous pipeline and lower the terms of documents that are very common\nidf = IDF().\\\n    setInputCol('tf').\\\n    setOutputCol('tfidf')\nidf_pipeline = Pipeline(stages=[cv_pipeline, idf]).fit(sarcasm_df)\nidf_pipeline.transform(sarcasm_df).show(5)\ntfidf_df = idf_pipeline.transform(sarcasm_df)\n\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#creating a logistic model with a= 0, lambda= 0.1 for model 1\nlr1 = LogisticRegression().\\\n    setLabelCol('sarcastic').\\\n    setFeaturesCol('tfidf').\\\n    setRegParam(0.1).\\\n    setMaxIter(100).\\\n    setElasticNetParam(0.)\n#creating a pipeline for model 1\nlr_pipeline = Pipeline(stages=[idf_pipeline, lr1]).fit(training)\n#estimating accuracy of model 1\nlr_pipeline.transform(validation).\\\n    select(fn.expr('float(prediction = sarcastic)').alias('correct')).\\\n    select(fn.avg('correct')).show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["lr_pipeline = Pipeline(stages=[idf_pipeline, lr1]).fit(validation)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(lr_pipeline.stages[-1])"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#creating a logistic model with a= 0.05, lambda= 0.1 for model 2\nlr2 = LogisticRegression().\\\n    setLabelCol('sarcastic').\\\n    setFeaturesCol('tfidf').\\\n    setRegParam(0.1).\\\n    setMaxIter(100).\\\n    setElasticNetParam(0.05)\n#creating a pipeline for model 2\nlr_pipeline = Pipeline(stages=[idf_pipeline, lr2]).fit(training)\n#estimating accuracy of model 2\nlr_pipeline.transform(validation).\\\n    select(fn.expr('float(prediction = sarcastic)').alias('correct')).\\\n    select(fn.avg('correct')).show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#creating a logistic model with a= 0.1, lambda= 0.1 for model 3\nlr3 = LogisticRegression().\\\n    setLabelCol('sarcastic').\\\n    setFeaturesCol('tfidf').\\\n    setRegParam(0.1).\\\n    setMaxIter(100).\\\n    setElasticNetParam(0.1)\n#creating a pipeline for model 3\nlr_pipeline = Pipeline(stages=[idf_pipeline, lr3]).fit(training)\n#estimating accuracy of model 3\nlr_pipeline.transform(validation).\\\n    select(fn.expr('float(prediction = sarcastic)').alias('correct')).\\\n    select(fn.avg('correct')).show()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#showing weights of best model 3 in elastic net regularztaion\n# show weights\nimport pandas as pd\nvocabulary = idf_pipeline.stages[0].stages[-1].vocabulary\nweights = lr_pipeline.stages[-1].coefficients.toArray()\ncoeffs_df = pd.DataFrame({'word': vocabulary, 'weight': weights})\n#lowest weighted words\ncoeffs_df.sort_values('weight').head(5)\n#highest weighted words\ncoeffs_df.sort_values('weight', ascending=False).head(5)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["**Question 3** (10 pts) Using the same features on the same training, validation, and testing splits from the previous question, fit a random forest classifier with 500 trees and evaluate its performance using accuracy. Is it better or worse than elatic net logistic regression?"],"metadata":{}},{"cell_type":"code","source":["# your code here\nfrom pyspark.ml.classification import RandomForestClassifier\n#creating random forest classifier\nrf = RandomForestClassifier(numTrees=500).\\\nsetLabelCol('sarcastic').\\\n    setFeaturesCol('tfidf')\nrf_pipeline = Pipeline(stages=[idf_pipeline, rf]).fit(training)\n#calculating accuracy\nrf_pipeline.transform(validation).\\\n    select(fn.expr('float(prediction = sarcastic)').alias('correct')).\\\n    select(fn.avg('correct')).show()\n#random forest classifier is  better than elastic net regression since it has an  increase in accuracy"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["**Question 4** (20 pts) Suppose that you want to catch every sarcastic tweet out here and therefore you care about high _recall_ rather than accuracy. Re-run the models from questions 1, 2 and 3 and estimate their recall on validation. Is the recall criteria changing which model is the best compared to the accuracy criteria?"],"metadata":{}},{"cell_type":"code","source":["# your code\ndef recall(df):\n  \n    tp = df[(df.sarcastic == 1) & (df.predicted == 1)].count()\n    fn = df[(df.sarcastic == 1) & (df.predicted == 0)].count()\n    recall= float(tp)/(tp+fn)\n    print recall"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["#recall of model in question 1\nprint(recall(predicted_sarcasm_df))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["def recall(df):\n  \n    tp = df[(df.sarcastic == 1) & (df.prediction== 1)].count()\n    fn = df[(df.sarcastic == 1) & (df.prediction == 0)].count()\n    recall= float(tp)/(tp+fn)\n    print recall"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#recall of best model for  elatsic net regularization\nrecall(lr_pipeline.transform(validation))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["#recall of random forest model\nrecall(rf_pipeline.transform(validation))\n#recall on validation is changing with models, the best model compared to accuracy is model 3 of elastic net regularization"],"metadata":{},"outputs":[],"execution_count":26}],"metadata":{"name":"homework3-no-key (1)","notebookId":885826825771445},"nbformat":4,"nbformat_minor":0}
